---
description: å¤§è¯­è¨€æ¨¡å‹ä½¿ç”¨
---

# ChatGLM-Using

## å‰è¨€

è‡ªä»OpenAI ChatGPTçˆ†ç«ä»¥æ¥ï¼Œå¼€æºç•Œ AIç•Œåˆ®èµ·äº†å¤§æ¨¡å‹ä¹‹é£ã€‚è¿™äº›å›½äº§å¤§æ¨¡å‹ï¼Œæˆ‘ä»¬å½“ç„¶è¦ä½“éªŒèµ·æ¥ã€‚æœ¬æ–‡ä¸»è¦ä»ä»£ç å±‚é¢è®°å½•äº†å¦‚ä½•è°ƒç”¨ä½“éªŒå¤§æ¨¡å‹ï¼Œä»ä¸€ä¸ªè°ƒåŒ…å·¥ç¨‹å¸ˆå‡çº§æˆä¸ºä¸€ä¸ªè°ƒç”¨å¤§æ¨¡å‹æ¨¡å‹å·¥ç¨‹å¸ˆã€‚:poop:

## ChatGLM2-6B

ChatGLM2-6Bæ˜¯ç”±æ¸…åTHUDMå®éªŒå®¤å¼€æºçš„ä¸­è‹±åŒè¯­å¯¹è¯æ¨¡å‹ [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) çš„ç¬¬äºŒä»£ç‰ˆï¼Œæœ¬åŸºäº [General Language Model (GLM)](https://github.com/THUDM/GLM) æ¶æ„ï¼Œå…·æœ‰ 62 äº¿å‚æ•°ã€‚ç»“åˆæ¨¡å‹é‡åŒ–æŠ€æœ¯ï¼Œç”¨æˆ·å¯ä»¥åœ¨æ¶ˆè´¹çº§çš„æ˜¾å¡ä¸Šè¿›è¡Œæœ¬åœ°éƒ¨ç½²ï¼ˆINT4 é‡åŒ–çº§åˆ«ä¸‹æœ€ä½åªéœ€ 6GB æ˜¾å­˜ï¼‰ã€‚ ChatGLM-6B ä½¿ç”¨äº†å’Œ ChatGPT ç›¸ä¼¼çš„æŠ€æœ¯ï¼Œé’ˆå¯¹ä¸­æ–‡é—®ç­”å’Œå¯¹è¯è¿›è¡Œäº†ä¼˜åŒ–ã€‚ç»è¿‡çº¦ 1T æ ‡è¯†ç¬¦çš„ä¸­è‹±åŒè¯­è®­ç»ƒï¼Œè¾…ä»¥ç›‘ç£å¾®è°ƒã€åé¦ˆè‡ªåŠ©ã€äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ç­‰æŠ€æœ¯çš„åŠ æŒï¼Œ62 äº¿å‚æ•°çš„ ChatGLM-6B å·²ç»èƒ½ç”Ÿæˆç›¸å½“ç¬¦åˆäººç±»åå¥½çš„å›ç­”ã€‚

### Reference

> 1. [https://github.com/THUDM/ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B)
> 2. [https://huggingface.co/THUDM/chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b)
> 3. [https://huggingface.co/THUDM/chatglm-6b-int4](https://huggingface.co/THUDM/chatglm-6b-int4)



### HardWare Requirements



<table><thead><tr><th width="149.33333333333331">é‡åŒ–ç­‰çº§</th><th width="277">ç¼–ç  2048 é•¿åº¦çš„æœ€å°æ˜¾å­˜</th><th>ç”Ÿæˆ 8192 é•¿åº¦çš„æœ€å°æ˜¾å­˜</th></tr></thead><tbody><tr><td>FP16 / BF16</td><td>13.1 GB</td><td>12.8 GB</td></tr><tr><td>INT8</td><td>8.2 GB</td><td>8.1 GB</td></tr><tr><td>INT4</td><td>5.5 GB</td><td>5.1 GB</td></tr></tbody></table>

è¿™é‡Œå—é™äºèµ„æºï¼Œæˆ‘ä»¬ä»¥é‡åŒ–INT4çš„æ¨¡å‹æµ‹è¯•ï¼ŒåŒæ—¶INT4æ ¹æ®å®˜ç½‘ä»‹ç»æ”¯æŒCPUã€‚



### DownLoad Models

è¿™é‡Œç›´æ¥ä¸‹è½½Hugging Face(HF)ä¸Šçš„inté‡åŒ–åçš„æ¨¡å‹ã€‚HFæ˜¯ä¸€ä¸ªæ‰˜ç®¡å‘å¸ƒåˆ†äº«æ¨¡å‹æ•°æ®é›†çš„å¼€æºå¹³å°ã€‚

```shell
# æ”¯æŒå¤§æ–‡ä»¶ä¸‹è½½
git lfs install
# ä¸‹è½½æ¨¡å‹ï¼Œçº¦7GBï¼Œä¸‹è½½æ¯”è¾ƒè€—æ—¶
git clone --depth 1 https://huggingface.co/THUDM/chatglm-6b-int4
```

å®‰è£…ä¾èµ–

```shell
# å®‰è£…ä¾èµ–ï¼Œä¾èµ–æºè‡ªChatGLM2-6B-int4 HF readme
pip install protobuf transformers==4.30.2 cpm_kernels torch==2.0.1 gradio mdtex2html sentencepiece accelerate
```

### Test ChatGLM2

```python
import time
from transformers import AutoTokenizer, AutoModel
# è¿™é‡Œå°†THUDM/chatglm2-6b-int4 æ›´æ¢ä¸ºåˆšæ¨¡å‹ä¸‹è½½å­˜æ”¾çš„è·¯å¾„ï¼Œ
# å¦åˆ™ä¼šè‡ªåŠ¨å»é‡æ–°ä¸‹è½½æ¨¡å‹åˆ°~/.cache/ç›®å½•ä¸‹
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b-int4", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm2-6b-int4", trust_remote_code=True).half().cuda()
model = model.eval()
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)

t = time.time()
response, history = model.chat(tokenizer, "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", history=history)
print(response)
print(f'coast:{time.time() - t:.4f}s')
```

```
ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
ç¡çœ å¯¹èº«ä½“å¥åº·å’Œå¿ƒç†å¥åº·éå¸¸é‡è¦ã€‚å¦‚æœæ™šä¸Šç¡ä¸ç€ï¼Œå¯ä»¥å°è¯•ä»¥ä¸‹ä¸€äº›æ–¹æ³•:

1. æ”¾æ¾è‡ªå·±:åœ¨ç¡è§‰å‰æ•°ç»µç¾Šæˆ–åšä¸€äº›è½»æ¾çš„ä¼¸å±•è¿åŠ¨ï¼Œå¯ä»¥å¸®åŠ©æ”¾æ¾è‡ªå·±ã€‚ä¹Ÿå¯ä»¥å°è¯•ä½¿ç”¨æ¸©å’Œçš„ç‘œä¼½æˆ–ä¼¸å±•ç»ƒä¹ ï¼Œç¼“è§£èº«ä½“çš„ç´§å¼ ã€‚

2. åˆ›å»ºä¸€ä¸ªèˆ’é€‚çš„ç¡çœ ç¯å¢ƒ:ç¡®ä¿å§å®¤å®‰é™ã€é»‘æš—ã€å‡‰çˆ½å’Œèˆ’é€‚ã€‚å¦‚æœå§å®¤ä¸­å­˜åœ¨å™ªéŸ³æˆ–å¼ºå…‰ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨è€³å¡æˆ–çœ¼ç½©ã€‚

3. é¿å…ä½¿ç”¨ç”µå­äº§å“:åœ¨ç¡è§‰å‰ä¸€å°æ—¶é¿å…ä½¿ç”¨ç”µå­äº§å“ï¼Œå¦‚æ‰‹æœºã€ç”µè§†æˆ–ç”µè„‘ã€‚è¿™äº›è®¾å¤‡å‘å‡ºçš„è“å…‰ä¼šå½±å“ç¡çœ è´¨é‡ã€‚

4. å»ºç«‹ä¸€ä¸ªå›ºå®šçš„ç¡çœ æ—¶é—´è¡¨:ä¿æŒå›ºå®šçš„ç¡çœ æ—¶é—´è¡¨å¯ä»¥å¸®åŠ©èº«ä½“å»ºç«‹ä¸€ä¸ªæ­£å¸¸çš„ç¡çœ èŠ‚å¾‹ã€‚å°½é‡åœ¨æ¯å¤©ç›¸åŒçš„æ—¶é—´ä¸ŠåºŠå’Œèµ·åºŠã€‚

5. é¿å…é¥®ç”¨åˆºæ¿€æ€§é¥®æ–™:åœ¨ç¡è§‰å‰å‡ å°æ—¶é¿å…é¥®ç”¨å’–å•¡ã€èŒ¶ã€é…’æˆ–å«æœ‰å·§å…‹åŠ›ç­‰åˆºæ¿€æ€§é¥®æ–™ã€‚

å¦‚æœè¿™äº›æ–¹æ³•æ— æ•ˆï¼Œå¯ä»¥è€ƒè™‘å’¨è¯¢åŒ»ç”Ÿä»¥äº†è§£å¯èƒ½çš„åŸå› ã€‚
coast:12.4483s
```

è¿™æ ·æˆ‘ä»¬å°±ä½¿ç”¨å®˜æ–¹çš„æ ·ä¾‹ä»£ç ç®€å•çš„ä½¿ç”¨ChatGLMè¿›è¡Œæ¨ç†å¯¹è¯äº†ã€‚æ¥ä¸‹æ¥è®©æˆ‘ä»¬å¯ä»¥ç®€å•çš„å°è£…ä¸€ä¸‹ï¼Œæ›´ä¼˜ç¾åœ°ä½¿ç”¨ChatGLM2ï¼

## FastAPI Deploy ChatGLM2

ä¸Šé¢æˆ‘ä»¬å·²ç»å­¦ä¼šäº†ä¸‹è½½æ¨¡å‹ï¼Œç„¶åè¿›è¡Œç®€å•çš„æ¨¡å‹å¯¹è¯æ¨ç†ã€‚æ¥ä¸‹æ¥ä½¿ç”¨æˆ‘ä»¬ä½¿ç”¨FastAPIå¯ä»¥å°†æ¨¡å‹éƒ¨ç½²åˆ°æœåŠ¡å™¨ä¸Šï¼Œä»¥æ¥å£æ–¹å¼è¿›è¡Œå¯¹è¯ã€‚æ³¨æ„è¿™é‡Œçš„APIä»…ä»…æ˜¯ç®€å•å°è£…ï¼Œä¸åŒäºFastChatç±»OpenAPIæä¾›çš„æ¥å£ã€‚

> FastAPI: [https://fastapi.tiangolo.com/](https://fastapi.tiangolo.com/)

```python
# -*-coding:utf-8-*-
'''
File Name:chatglm2-6b-stream-api.py
Author:LongerKing
Time:2023/8/15 13:33
'''

import os
import sys
import json
import torch
import uvicorn
import logging
import argparse
from fastapi import FastAPI
from transformers import AutoTokenizer, AutoModel
from fastapi.middleware.cors import CORSMiddleware
from sse_starlette.sse import ServerSentEvent, EventSourceResponse


def getLogger(name, file_name, use_formatter=True):
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)
    console_handler = logging.StreamHandler(sys.stdout)
    formatter = logging.Formatter('%(asctime)s    %(message)s')
    console_handler.setFormatter(formatter)
    console_handler.setLevel(logging.INFO)
    logger.addHandler(console_handler)
    if file_name:
        handler = logging.FileHandler(file_name, encoding='utf8')
        handler.setLevel(logging.INFO)
        if use_formatter:
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(message)s')
            handler.setFormatter(formatter)
        logger.addHandler(handler)
    return logger


logger = getLogger('ChatGLM', 'chatlog.log')

MAX_HISTORY = 3


class ChatGLM():
    def __init__(self) -> None:
        logger.info("Start initialize model...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            "/home/bigdata/wonders/wanglang/models/chatglm2-6b-cmeee_eie001", trust_remote_code=True)
        self.model = AutoModel.from_pretrained("/home/bigdata/wonders/wanglang/models/chatglm2-6b-cmeee_eie001",
                                               trust_remote_code=True).quantize(8).cuda()
        self.model.eval()
        logger.info("Model initialization finished.")

    def clear(self) -> None:
        if torch.cuda.is_available():
            with torch.cuda.device(f"cuda:{args.device}"):
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()

    def answer(self, query: str, history):
        response, history = self.model.chat(self.tokenizer, query, history=history)
        history = [list(h) for h in history]
        return response, history

    def stream(self, query, history):
        if query is None or history is None:
            yield {"query": "", "response": "", "history": [], "finished": True}
        size = 0
        response = ""
        for response, history in self.model.stream_chat(self.tokenizer, query, history):
            this_response = response[size:]
            history = [list(h) for h in history]
            size = len(response)
            yield {"delta": this_response, "response": response, "finished": False}
        logger.info("Answer - {}".format(response))
        yield {"query": query, "delta": "[EOS]", "response": response, "history": history, "finished": True}


def start_server(http_address: str, port: int, gpu_id: str):
    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
    os.environ['CUDA_VISIBLE_DEVICES'] = gpu_id

    bot = ChatGLM()

    app = FastAPI()
    app.add_middleware(CORSMiddleware,
                       allow_origins=["*"],
                       allow_credentials=True,
                       allow_methods=["*"],
                       allow_headers=["*"]
                       )

    @app.get("/")
    def index():
        return {'message': 'started', 'success': True}

    @app.post("/chat")
    async def answer_question(arg_dict: dict):
        result = {"query": "", "response": "", "success": False}
        try:
            text = arg_dict["query"]
            ori_history = arg_dict["history"]
            logger.info("Query - {}".format(text))
            if len(ori_history) > 0:
                logger.info("History - {}".format(ori_history))
            history = ori_history[-MAX_HISTORY:]
            history = [tuple(h) for h in history]
            response, history = bot.answer(text, history)
            logger.info("Answer - {}".format(response))
            ori_history.append((text, response))
            result = {"query": text, "response": response,
                      "history": ori_history, "success": True}
        except Exception as e:
            logger.error(f"error: {e}")
        return result

    @app.post("/stream")
    def answer_question_stream(arg_dict: dict):
        def decorate(generator):
            for item in generator:
                yield ServerSentEvent(json.dumps(item, ensure_ascii=False), event='delta')

        try:
            text = arg_dict["query"]
            ori_history = arg_dict["history"]
            logger.info("Query - {}".format(text))
            if len(ori_history) > 0:
                logger.info("History - {}".format(ori_history))
            history = ori_history[-MAX_HISTORY:]
            history = [tuple(h) for h in history]
            return EventSourceResponse(decorate(bot.stream(text, history)))
        except Exception as e:
            logger.error(f"error: {e}")
            return EventSourceResponse(decorate(bot.stream(None, None)))

    @app.get("/free_gc")
    def free_gpu_cache():
        try:
            bot.clear()
            return {"success": True}
        except Exception as e:
            logger.error(f"error: {e}")
            return {"success": False}

    logger.info("starting server...")
    uvicorn.run(app=app, host=http_address, port=port, workers=1)


if __name__ == '__main__':

    parser = argparse.ArgumentParser(description='Stream API Service for ChatGLM2-6B')
    parser.add_argument('--device', '-d', help='deviceï¼Œ-1 means cpu, other means gpu ids', default='0')
    parser.add_argument('--host', '-H', help='host to listen', default='0.0.0.0')
    parser.add_argument('--port', '-P', help='port of this service', default=8000)
    args = parser.parse_args()
    start_server(args.host, int(args.port), args.device)
```
